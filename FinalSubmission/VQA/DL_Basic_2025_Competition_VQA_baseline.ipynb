{"cells":[{"cell_type":"markdown","metadata":{"id":"rrgnykefNSGU"},"source":["# Deep Learning 基礎講座　最終課題: VQA\n","\n","## 概要\n","画像と質問から，回答を予測するタスクです．．\n","- サンプル数: 訓練 19,873 サンプル，テスト 4,969 サンプル\n","- 入力: 画像データ（RGB，サイズは画像によって異なります），質問文（系列長はサンプルごとに異なります）\n","- 出力: 回答文（系列長はサンプルごとに異なります）\n","- 評価指標: VQA での評価指標（[こちら\n","](https://visualqa.org/evaluation.html)を参照）を利用しています．\n","\n","### データセット ([VizWiz 2023 edition](https://www.kaggle.com/datasets/nqa112/vizwiz-2023-edition)) の詳細\n","- 24,842 枚の画像データセットと，各画像に対する 1 つの質問文と 10 人の回答者による回答文から構成されます．\n","  - 10 人の回答は全て同じとは限りません．\n","- 24.842 サンプルのうち，80 % (19.873) が訓練データ (train)，20 % (4969) がテストデータ (val) として与えられます．\n","  - テストデータに対する回答文を正解ラベルとし，配布していません．\n","  - データ提供元とは異なるデータ分割になっています．\n","\n","### タスクの詳細\n","- 本コンペティションでは，与えられた画像と質問文に対して，適切な回答文を出力するモデルを作成していただきます．\n","- 評価は [VQA](https://visualqa.org/index.html) (Visual Question Answering) に基づいて，以下の式で計算されます．\n","\n","$$\\text{Acc}(ans) = \\text{min}(\\frac{humans \\; that \\; said \\; ans}{3}, 1)$$\n","\n","- 1 つのデータに対し， 10 人の回答のうち 9 人の回答を選択し上記の式で性能評価した， 10 パターンの Acc の平均をそのデータに対する Acc とします．\n","- 予測結果と正解ラベルを比較する前に，回答を lowercase にする，冠詞は削除するなどの前処理を行っています（[詳細](https://visualqa.org/evaluation.html)）．"]},{"cell_type":"markdown","metadata":{"id":"E3gJ3WWqNRTJ"},"source":["## 考えられる工夫の例\n","- 事前学習モデルの fine-tuning\n","    - 画像特徴量，言語特徴量を取得するときに，事前学習モデルを fine-tuning することで性能向上が見込めます（今回のタスクと大きく異なるデータセットでの事前学習では効果が小さい可能性がありますので注意しましょう）．\n","- 質問文の表現\n","    - ベースラインでは，質問文をモデルに入力する際に，one-hot ベクトルにしています．これを tokenizer 等を利用して分散表現にすることで，モデル学習しやすくなります．\n","- ソフトラベルの利用\n","    - ベースラインでは 10 人の回答の中で最も多かった回答を正解ラベルとして訓練しています．この点を各回答の頻度に合わせてソフトラベルを利用することで，より多くの情報を利用して学習が可能になります．\n","- 画像の前処理\n","    - 画像の前処理には形状を同じにする Resize のみを利用しています．「畳み込みニューラルネットワーク」，「深層学習と画像認識」等で紹介されていたデータ拡張を追加することで，汎化性能の向上が見込めます．"]},{"cell_type":"markdown","metadata":{"id":"Ko1FwYB2qjBp"},"source":["## 修了要件を満たす条件\n","- ベースラインでは，omnicampus 上での性能評価において， 49.3% となります．したがって，ベースラインを超える 49.4% 以上の提出のみ，修了要件として認めます．\n","- ベースラインから改善を加えることで， 60% に性能向上することを運営で確認しています．こちらを 1 つの指標として取り組んでみてください．"]},{"cell_type":"markdown","metadata":{"id":"RtEm3kflpowF"},"source":["## 注意点\n","- 学習するモデルについて制限はありませんが，必ず訓練データで学習したモデルで予測してください．\n","    - 事前学習済みモデルを利用して，訓練データを fine-tuning しても構いません．\n","    - 埋め込み抽出モデルなど，モデルの一部を訓練しないケースは構いません．\n","    - 学習を一切せずに，ChatGPT などの基盤モデルを利用することは禁止とします．"]},{"cell_type":"markdown","metadata":{"id":"4YOIezNNg1S-"},"source":["### データの準備\n","データをダウンロードした際に，google drive したため，利用するために google drive をマウントする必要があります．また， drive 上で展開することができないため，/content ディレクトリ下にコピーし \"data.zip\" を展開します．  \n","google drive 上に \"data.zip\" が配置されていない場合は実行できません．google drive 上に \"data.zip\" (**12GB**) を配置することが可能であれば，\"data_download.ipynb\" を先に実行してください．難しい場合は，omnicampus 演習環境を利用してください．．\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWZZFP4UW0Rw"},"outputs":[],"source":["# omnicampus 上では 4 セル目まで実行不要\n","# ドライブのマウント\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PL8LLFikYN2q"},"outputs":[],"source":["# データダウンロード用の notebook にてgoogle drive への保存後，\n","# 反映に時間がかかる可能性がありますので，google drive のマウント後，\n","# data.zip がディレクトリ内にあることを確認してから実行してください．\n","# data.zip を /content 下にコピーする\n","!cp \"/content/drive/MyDrive/data.zip\" \"/content\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O7pkQvmZ1wW"},"outputs":[],"source":["# カレントディレクトリ下のファイル群を確認\n","# data.zip が表示されれば問題ないです\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEae3j7d6Iuc"},"outputs":[],"source":["# データを解凍する\n","!unzip data.zip"]},{"cell_type":"markdown","metadata":{"id":"ijF6qw6IwkFh"},"source":["omnicampus 演習環境では，data_download.ipynb のマウント，zip 化，drive へのコピーを実行しないことで，\"data.zip\" を解凍した形で配置されます．したがって，data ディレクトリが存在するディレクトリをカレントディレクトリとするだけで良いです．\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6ZGrJT7w_wV"},"outputs":[],"source":["# omnicampus 実行用\n","# 以下の例では/workspace に data ディレクトリがあると想定\n","%cd /workspace/VQA"]},{"cell_type":"markdown","metadata":{"id":"vLSfnK-rZuwl"},"source":["### 1. import library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peG6AR64aD0G"},"outputs":[],"source":["import re\n","import random\n","import time\n","from statistics import mode\n","\n","from PIL import Image\n","import numpy as np\n","import pandas\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms"]},{"cell_type":"markdown","metadata":{"id":"9YOJqWpSaGGL"},"source":["### 2. utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKpVLucPaHj4"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"\n","    シードを固定する．\n","\n","    Parameters\n","    ----------\n","    seed : int\n","        乱数生成に用いるシード値．\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MD2OngWUaPYy"},"outputs":[],"source":["def process_text(text):\n","    \"\"\"\n","    入力文と回答のフォーマットを統一するための関数．\n","\n","    Parameters\n","    ----------\n","    text : str\n","        入力文，もしくは回答．\n","    \"\"\"\n","    # lowercase\n","    text = text.lower()\n","\n","    # 数詞を数字に変換\n","    num_word_to_digit = {\n","        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n","        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n","        'ten': '10'\n","    }\n","    for word, digit in num_word_to_digit.items():\n","        text = text.replace(word, digit)\n","\n","    # 小数点のピリオドを削除\n","    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n","\n","    # 冠詞の削除\n","    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n","\n","    # 短縮形のカンマの追加\n","    contractions = {\n","        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n","        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n","    }\n","    for contraction, correct in contractions.items():\n","        text = text.replace(contraction, correct)\n","\n","    # 句読点をスペースに変換\n","    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n","\n","    # 句読点をスペースに変換\n","    text = re.sub(r'\\s+,', ',', text)\n","\n","    # 連続するスペースを1つに変換\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tTxwYSKaoOO"},"outputs":[],"source":["class VQADataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    VQA データセットを扱うためのクラス．\n","    \"\"\"\n","    def __init__(self, df_path, image_dir, transform=None, answer=True):\n","        self.transform = transform  # 画像の前処理\n","        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n","        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n","        self.answer = answer\n","\n","        # question / answerの辞書を作成\n","        self.question2idx = {}\n","        self.answer2idx = {}\n","        self.idx2question = {}\n","        self.idx2answer = {}\n","\n","        # 質問文に含まれる単語を辞書に追加\n","        for question in self.df[\"question\"]:\n","            question = process_text(question)\n","            words = question.split(\" \")\n","            for word in words:\n","                if word not in self.question2idx:\n","                    self.question2idx[word] = len(self.question2idx)\n","        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n","\n","        if self.answer:\n","            # 回答に含まれる文章を辞書に追加\n","            for answers in self.df[\"answers\"]:\n","                for answer in answers:\n","                    word = answer[\"answer\"]\n","                    word = process_text(word)\n","                    if word not in self.answer2idx:\n","                        self.answer2idx[word] = len(self.answer2idx)\n","            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n","\n","    def update_dict(self, dataset):\n","        \"\"\"\n","        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n","\n","        Parameters\n","        ----------\n","        dataset : Dataset\n","            訓練データのDataset\n","        \"\"\"\n","        self.question2idx = dataset.question2idx\n","        self.answer2idx = dataset.answer2idx\n","        self.idx2question = dataset.idx2question\n","        self.idx2answer = dataset.idx2answer\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        対応するidxのデータ（画像，質問，回答）を取得．\n","\n","        Parameters\n","        ----------\n","        idx : int\n","            取得するデータのインデックス\n","\n","        Returns\n","        -------\n","        image : torch.Tensor  (C, H, W)\n","            画像データ\n","        question : torch.Tensor  (vocab_size)\n","            質問文をone-hot表現に変換したもの\n","        answers : torch.Tensor  (n_answer)\n","            10人の回答者の回答のid\n","        mode_answer_idx : torch.Tensor  (1)\n","            10人の回答者の回答の中で最頻値の回答のid\n","        \"\"\"\n","        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n","        image = self.transform(image)\n","        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n","        question_words = self.df[\"question\"][idx].split(\" \")\n","        for word in question_words:\n","            try:\n","                question[self.question2idx[word]] = 1  # one-hot表現に変換\n","            except KeyError:\n","                question[-1] = 1  # 未知語\n","\n","        if self.answer:\n","            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n","            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n","\n","            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx)\n","\n","        else:\n","            return image, torch.Tensor(question)\n","\n","    def __len__(self):\n","        return len(self.df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f83T-sQca0DC"},"outputs":[],"source":["def VQA_criterion(batch_pred, batch_answers):\n","    \"\"\"\n","    VQA タスクに用いられる評価関数．\n","    \"\"\"\n","    total_acc = 0.\n","\n","    for pred, answers in zip(batch_pred, batch_answers):\n","        acc = 0.\n","        for i in range(len(answers)):\n","            num_match = 0\n","            for j in range(len(answers)):\n","                if i == j:\n","                    continue\n","                if pred == answers[j]:\n","                    num_match += 1\n","            acc += min(num_match / 3, 1)\n","        total_acc += acc / 10\n","\n","    return total_acc / len(batch_pred)"]},{"cell_type":"markdown","metadata":{"id":"afULfdGfa9BF"},"source":["### 3. model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8bOkolIbBBg"},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    \"\"\"\n","    ResNet の basic block\n","    \"\"\"\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        in_channles: int\n","            入力のチャネル数\n","        out_channels:\n","            出力のチャネル数\n","        stride: int\n","            ストライド\n","        \"\"\"\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        順伝播処理\n","\n","        Parameters\n","        ----------\n","        x: torch.Tensor\n","            ブロックへの入力\n","\n","        Returns\n","        -------\n","        out: torch.Tensor\n","            ブロックへの出力\n","        \"\"\"\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class BottleneckBlock(nn.Module):\n","    \"\"\"\n","    ResNet の bottleneck block\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        in_channles: int\n","            入力のチャネル数\n","        out_channels:\n","            出力のチャネル数\n","        stride: int\n","            ストライド\n","        \"\"\"\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n","        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels * self.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels * self.expansion)\n","            )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        順伝播処理\n","\n","        Parameters\n","        ----------\n","        x: torch.Tensor\n","            ブロックへの入力\n","\n","        Returns\n","        -------\n","        out: torch.Tensor\n","            ブロックへの出力\n","        \"\"\"\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    \"\"\"\n","    ResNet の実装\n","    \"\"\"\n","    def __init__(self, block, layers):\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        block: torch.nn.Module\n","            利用するブロックのクラス (BasicBlock / BottleneckBlock)\n","        layers: list\n","            各ブロックの層数\n","        \"\"\"\n","        super().__init__()\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(block, layers[0], 64)\n","        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n","        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n","        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, 512)\n","\n","    def _make_layer(self, block, blocks, out_channels, stride=1):\n","        \"\"\"\n","        同じ構成を繰り返す部分を生成する．\n","\n","        Parameters\n","        ----------\n","        block: torch.nn.Module\n","            利用するブロックのクラス (BasicBlock / BottleneckBlock)\n","        blocks: int\n","            層数\n","        out_channels: int\n","            出力のチャネル数\n","        stride: int\n","            ストライド\n","\n","        Returns\n","        -------\n","        layers: torch.nn.ModuleList\n","            生成した層\n","        \"\"\"\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride))\n","        self.in_channels = out_channels * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.in_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        順伝播処理\n","\n","        Parameters\n","        ----------\n","        x: torch.Tensor\n","            入力データ\n","\n","        Returns\n","        -------\n","        x: torch.Tensor\n","            ResNet によって生成される特徴量\n","        \"\"\"\n","        x = self.relu(self.bn1(self.conv1(x)))\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptR4SOabbG1-"},"outputs":[],"source":["def ResNet18():\n","    \"\"\"\n","    ResNet18 を生成する関数．\n","    \"\"\"\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","\n","def ResNet50():\n","    \"\"\"\n","    ResNet50 を生成する関数．\n","    \"\"\"\n","    return ResNet(BottleneckBlock, [3, 4, 6, 3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4NR2fhDbID_"},"outputs":[],"source":["class VQAModel(nn.Module):\n","    \"\"\"\n","    VQA タスクを解くためのモデル例．\n","    \"\"\"\n","    def __init__(self, vocab_size: int, n_answer: int):\n","        \"\"\"\n","        コンストラクタ．\n","\n","        Parameters\n","        ----------\n","        vocab_size: int\n","            入力文の語彙数\n","        n_answer: int\n","            出力のクラス数\n","        \"\"\"\n","        super().__init__()\n","        self.resnet = ResNet18()\n","        self.text_encoder = nn.Linear(vocab_size, 512)\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, n_answer)\n","        )\n","\n","    def forward(self, image, question):\n","\n","        image_feature = self.resnet(image)  # 画像の特徴量\n","        question_feature = self.text_encoder(question)  # テキストの特徴量\n","\n","        x = torch.cat([image_feature, question_feature], dim=1)\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"cKFXLwakbMmq"},"source":["### 4. train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxFHy9ozbNo9"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, device):\n","    \"\"\"\n","    学習用の関数．\n","\n","    Parameters\n","    ----------\n","    model: torch.nn.Module\n","        学習するモデル\n","    dataloader: torch.utils.data.DataLoader\n","        学習に利用するデータローダ\n","    optimizer: torch.optim.Optim\n","        最適化手法\n","    criterion: torch.nn.Module\n","        損失関数\n","    device: torch.device\n","        学習に利用するデバイス\n","\n","    Returns\n","    -------\n","    total_loss: float\n","        平均損失\n","    total_acc: float\n","        平均正解率\n","    simple_acc: float\n","        最頻値に対する正解率（VQA の評価指標とは異なることに注意）\n","    time: float\n","        1 エポックの学習にかかった時間 (sec)\n","    \"\"\"\n","    model.train()\n","\n","    total_loss = 0\n","    total_acc = 0\n","    simple_acc = 0\n","\n","    start = time.time()\n","    for image, question, answers, mode_answer in dataloader:\n","        image, question, answer, mode_answer = \\\n","            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n","\n","        pred = model(image, question)\n","        loss = criterion(pred, mode_answer.squeeze())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n","        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n","\n","    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n","\n","\n","def eval(model, dataloader, optimizer, criterion, device):\n","    \"\"\"\n","    学習用の関数．\n","\n","    Parameters\n","    ----------\n","    model: torch.nn.Module\n","        モデル\n","    dataloader: torch.utils.data.DataLoader\n","        評価に利用するデータローダ\n","    criterion: torch.nn.Module\n","        損失関数\n","    device: torch.device\n","        利用するデバイス\n","\n","    Returns\n","    -------\n","    total_loss: float\n","        平均損失\n","    total_acc: float\n","        平均正解率\n","    simple_acc: float\n","        最頻値に対する正解率（VQA の評価指標とは異なることに注意）\n","    time: float\n","        1 エポックの評価にかかった時間 (sec)\n","    \"\"\"\n","    model.eval()\n","\n","    total_loss = 0\n","    total_acc = 0\n","    simple_acc = 0\n","\n","    start = time.time()\n","    for image, question, answers, mode_answer in dataloader:\n","        image, question, answer, mode_answer = \\\n","            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n","\n","        pred = model(image, question)\n","        loss = criterion(pred, mode_answer.squeeze())\n","\n","        total_loss += loss.item()\n","        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n","        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n","\n","    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"]},{"cell_type":"markdown","metadata":{"id":"fUbpNdDgbTgP"},"source":["### 5. make submission file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHF3chhFbkky"},"outputs":[],"source":["# deviceの設定\n","set_seed(42)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# dataloader / model\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\n","test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\n","test_dataset.update_dict(train_dataset)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","model = VQAModel(vocab_size=len(train_dataset.question2idx)+1, n_answer=len(train_dataset.answer2idx)).to(device)\n","\n","# optimizer / criterion\n","num_epoch = 4\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQopVAWZb1ee"},"outputs":[],"source":["# train model\n","for epoch in range(num_epoch):\n","    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n","    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n","            f\"train time: {train_time:.2f} [s]\\n\"\n","            f\"train loss: {train_loss:.4f}\\n\"\n","            f\"train acc: {train_acc:.4f}\\n\"\n","            f\"train simple acc: {train_simple_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mD-SpZwMb81q"},"outputs":[],"source":["# make submission file\n","model.eval()\n","submission = []\n","for image, question in test_loader:\n","    image, question = image.to(device), question.to(device)\n","    pred = model(image, question)\n","    pred = pred.argmax(1).cpu().item()\n","    submission.append(pred)\n","\n","submission = [train_dataset.idx2answer[id] for id in submission]\n","submission = np.array(submission)\n","torch.save(model.state_dict(), \"model.pt\")\n","np.save(\"submission.npy\", submission)"]},{"cell_type":"markdown","metadata":{"id":"JbEyP4q9b_6f"},"source":["## 提出方法\n","\n","以下の3点をzip化し，Omnicampusの「最終課題 (VQA)」から提出してください．\n","\n","- `submission.npy`\n","- `model.pt`や`model_best.pt`など，テストに使用した重み（拡張子は`.pt`のみ）\n","- 本Colab Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW3NA_ruqegr"},"outputs":[],"source":["from zipfile import ZipFile\n","\n","model_path = \"model.pt\"\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/DL_Basic_2025_Competition_VQA_baseline.ipynb\"\n","\n","with ZipFile(\"submission.zip\", \"w\") as zf:\n","    zf.write(\"submission.npy\")\n","    zf.write(model_path)\n","    zf.write(notebook_path, arcname=\"DL_Basic_2025_Competition_VQA_baseline.ipynb\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKRPwFaLFw1f"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}