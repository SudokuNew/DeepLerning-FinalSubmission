{"cells":[{"cell_type":"markdown","metadata":{"id":"jTJ_SWhK1MBp"},"source":["# Deep Learning 基礎講座　最終課題: NYUv2 セマンティックセグメンテーション\n","\n","## 概要\n","RGB画像から、画像内の各ピクセルがどのクラスに属するかを予測するセマンティックセグメンテーションタスク.\n","\n","### データセット\n","- データセット: NYUv2 dataset\n","- 訓練データ: 795枚\n","- テストデータ: 654枚\n","- 入力: RGB画像 + 深度マップ（元画像サイズは可変）\n","- 出力: 13クラスのセグメンテーションマップ\n","- 評価指標: Mean IoU (Intersection over Union)\n","\n","### データセットの詳細（[NYU Depth Dataset V2](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)）\n","- 画像は屋内シーンを撮影したもので、家具や壁、床などの物体が含まれています.\n","- 各画像に対して13クラスのセグメンテーションラベルが提供されます.\n","- データは以下のディレクトリ構造で提供:\n","```\n","data/NYUv2/\n","├─train/\n","│  ├─image/      # RGB画像\n","│  │    000000.png\n","│  │    ...\n","│  │\n","│  ├─depth/      # 深度マップ\n","│  │    000000.png\n","│  │    ...\n","│  │\n","│  └─label/      # 13クラスセグメンテーション（教師ラベル）\n","│       000000.png\n","│       ...\n","└─test/\n","   ├─image/      # RGB画像\n","   │    000000.png\n","   │    ...\n","   │  ├─depth/   # 深度マップ\n","   │    000000.png\n","   │    ...\n","```\n","\n","### タスクの詳細\n","- 入力のRGB画像と深度マップから、各ピクセルが13クラスのどれに属するかを予測するタスクです.\n","- 評価はMean IoUを使用します．\n","  - 各クラスごとにIoUを計算し、その平均を取ります.\n","  - IoUは以下の式で計算:\n","  $$IoU = \\frac{TP}{TP + FP + FN}$$\n","    - TP: True Positive（正しく予測されたピクセル数）\n","    - FP: False Positive（誤って予測されたピクセル数）\n","    - FN: False Negative（見逃したピクセル数）\n","\n","### 前処理\n","- 入力画像は512×512にリサイズされます.\n","- ピクセル値は0-1に正規化されます.\n","- セグメンテーションラベルは0-12の整数値（13クラス）です．\n","  - 255はignore index（評価から除外）\n","\n","### 提出形式\n","- テスト画像（RGB + Depth）の各ピクセルに対してクラス（0~12）を予測したものをnumpy配列として保存されます.\n","- ファイル名: `submission.npy`\n","- 配列の形状: [テストデータ数, 高さ, 幅]\n","- 各ピクセルの値: 0-12の整数（予測クラス）\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4ValDFnY5hc6"},"source":["## 考えられる工夫の例\n","- 事前学習モデルの fine-tuning\n","    - ImageNetなどで事前学習されたモデルを本データセットでfine-tuningすることで性能向上が見込めます.\n","- 損失関数の再設計\n","    - クラスごとの出現頻度に応じて損失を補正するように損失関数を設計すると、クラス分布の不均衡に対してロバストな学習ができます.\n","- 画像の前処理\n","    - RandomResizedCrop / Flip / ColorJitter 等のデータ拡張を追加することで，汎化性能の向上が見込めます．"]},{"cell_type":"markdown","metadata":{"id":"6r3PFWN_8bkJ"},"source":["## 修了要件を満たす条件\n","- ベースラインでは，omnicampus 上での性能評価において， 38.2% となります．したがって，ベースラインである 38.2% を超えた提出のみ，修了要件として認めます．\n","- ベースラインから改善を加えることで， 50%以上に性能向上することを運営で確認しています．こちらを 1つの指標として取り組んでみてください．"]},{"cell_type":"markdown","metadata":{"id":"k52rQxqy8fvT"},"source":["## 注意点\n","- 学習するモデルについて制限はありませんが，必ず訓練データで学習したモデルで予測してください．\n","    - 事前学習済みモデルを利用して，訓練データを fine-tuning しても構いません．\n","    - 埋め込み抽出モデルなど，モデルの一部を訓練しないケースは構いません．\n","    - 学習を一切せずに，ChatGPT などの基盤モデルを利用することは禁止とします．"]},{"cell_type":"markdown","metadata":{"id":"3qGzRC_T8iyS"},"source":["### データの準備\n","データをダウンロードした際に，google drive したため，利用するために google drive をマウントする必要があります．また， drive 上で展開することができないため，/content ディレクトリ下にコピーし \"data.zip\" を展開します．  \n","google drive 上に \"data.zip\" が配置されていない場合は実行できません．google drive 上に \"data.zip\" (**831MB**) を配置することが可能であれば，\"data_download.ipynb\" を先に実行してください．難しい場合は，omnicampus 演習環境を利用してください．．\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPPssX_4XXfn"},"outputs":[],"source":["# omnicampus 上では 4 セル目まで実行不要\n","# ドライブのマウント\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0Fl3d2HTRg9"},"outputs":[],"source":["# データダウンロード用の notebook にてgoogle drive への保存後，\n","# 反映に時間がかかる可能性がありますので，google drive のマウント後，\n","# data.zip がディレクトリ内にあることを確認してから実行してください．\n","# data.zip を /content 下にコピーする\n","!cp \"/content/drive/MyDrive/data.zip\" \"/content\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8tO1CHiTSOw"},"outputs":[],"source":["# カレントディレクトリ下のファイル群を確認\n","# data.zip が表示されれば問題ないです\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zCM4zSbTUbp"},"outputs":[],"source":["# データを解凍する\n","!unzip data.zip\n","!mkdir data\n","!mv train test data/"]},{"cell_type":"markdown","metadata":{"id":"YZmntYPt_ITA"},"source":["omnicampus 演習環境では，data_download.ipynb のマウント，zip 化，drive へのコピーを実行しないことで，\"data.zip\" を解凍した形で配置されます．したがって，data ディレクトリが存在するディレクトリをカレントディレクトリとするだけで良いです．\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbxoYYC9_ITB"},"outputs":[],"source":["# omnicampus 実行用\n","# 以下の例では/workspace/Segmentation/split_data_scripts/omnicampus に data ディレクトリがあると想定\n","%cd /workspace/Segmentation/split_data_scripts_omnicampus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKQQYWiO_ITB"},"outputs":[],"source":["!pip install numpy==1.22.2 h5py scikit-image"]},{"cell_type":"markdown","metadata":{"id":"_ps9z7F3E4zk"},"source":["# import library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DtvdZmhKh9t"},"outputs":[],"source":["import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Asqo5icz-gJR"},"outputs":[],"source":["import os\n","import time\n","from tqdm import tqdm\n","import numpy as np\n","from scipy.io import loadmat\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.utils.data as data\n","from torch.utils.data import random_split, DataLoader\n","from torchvision.datasets import VisionDataset\n","from torchvision import transforms\n","from torchvision.transforms import (\n","    Compose,\n","    RandomResizedCrop,\n","    RandomHorizontalFlip,\n","    ColorJitter,\n","    GaussianBlur,\n","    Resize,\n","    ToTensor,\n","    Normalize,\n","    Lambda,\n","    InterpolationMode\n",")\n","from torch.cuda.amp import autocast, GradScaler\n","from dataclasses import dataclass\n","import random"]},{"cell_type":"markdown","metadata":{"id":"Q8HLH2R0_dPn"},"source":["# DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38jvO_Nz_cXD"},"outputs":[],"source":["# カラーマップ生成関数：セグメンテーションの可視化用\n","def colormap(N=256, normalized=False):\n","    def bitget(byteval, idx):\n","        return ((byteval & (1 << idx)) != 0)\n","\n","    dtype = 'float32' if normalized else 'uint8'\n","    cmap = np.zeros((N, 3), dtype=dtype)\n","    for i in range(N):\n","        r = g = b = 0\n","        c = i\n","        for j in range(8):\n","            r = r | (bitget(c, 0) << 7-j)\n","            g = g | (bitget(c, 1) << 7-j)\n","            b = b | (bitget(c, 2) << 7-j)\n","            c = c >> 3\n","\n","        cmap[i] = np.array([r, g, b])\n","\n","    cmap = cmap/255 if normalized else cmap\n","    return cmap\n","\n","# NYUv2データセット：RGB画像、セグメンテーション、深度、法線マップを提供するデータセット\n","class NYUv2(VisionDataset):\n","    \"\"\"NYUv2 dataset\n","\n","    Args:\n","        root (string): Root directory path.\n","        split (string, optional): 'train' for training set, and 'test' for test set. Default: 'train'.\n","        target_type (string, optional): Type of target to use, ``semantic``, ``depth``.\n","        transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version.\n","        target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n","    \"\"\"\n","    cmap = colormap()\n","    def __init__(self,\n","                 root,\n","                 split='train',\n","                 include_depth=False,\n","                 transform=None,\n","                 target_transform=None,\n","                 ):\n","        super(NYUv2, self).__init__(root, transform=transform, target_transform=target_transform)\n","\n","        # データセットの基本設定\n","        assert(split in ('train', 'test'))\n","        self.root = root\n","        self.split = split\n","        self.include_depth = include_depth\n","        self.train_idx = np.array([255, ] + list(range(13)))  # 13クラス分類用\n","\n","        # 画像ファイルのパスリストを作成\n","        img_names = os.listdir(os.path.join(self.root, self.split, 'image'))\n","        img_names.sort()\n","        images_dir = os.path.join(self.root, self.split, 'image')\n","        self.images = [os.path.join(images_dir, name) for name in img_names]\n","\n","        label_dir = os.path.join(self.root, self.split, 'label')\n","        if (self.split == 'train'):\n","          self.labels = [os.path.join(label_dir, name) for name in img_names]\n","          self.targets = self.labels\n","\n","        depth_dir = os.path.join(self.root, self.split, 'depth')\n","        self.depths = [os.path.join(depth_dir, name) for name in img_names]\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.images[idx])\n","        depth = Image.open(self.depths[idx])\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            depth = self.transform(depth)\n","        if self.split=='test':\n","          if self.include_depth:\n","              return image, depth\n","          return image\n","        if self.split == 'train' and self.target_transform is not None:\n","            target = Image.open(self.targets[idx])\n","            target = self.target_transform(target)\n","        if self.include_depth:\n","              return image, depth, target\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"markdown","metadata":{"id":"TgWaxx12628b"},"source":["# Model Section\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3q03fys06aKJ"},"outputs":[],"source":["# 2つの畳み込み層とバッチ正規化、ReLUを含むブロック\n","# UNetの各層で使用される基本的な畳み込みブロック\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","# UNetモデル：エンコーダ・デコーダ構造のセグメンテーションモデル\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        # エンコーダ部分：特徴量の抽出と空間サイズの縮小\n","        self.enc1 = DoubleConv(in_channels, 64)\n","        self.enc2 = DoubleConv(64, 128)\n","        self.enc3 = DoubleConv(128, 256)\n","        self.enc4 = DoubleConv(256, 512)\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # デコーダ部分：特徴量の統合と空間サイズの復元\n","        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","        self.dec3 = DoubleConv(512 + 256, 256)\n","        self.dec2 = DoubleConv(256 + 128, 128)\n","        self.dec1 = DoubleConv(128 + 64, 64)\n","\n","        # 最終層：クラス数に応じた出力チャネルに変換\n","        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        # エンコーダパス：特徴抽出とダウンサンプリング\n","        e1 = self.enc1(x)\n","        e2 = self.enc2(self.pool(e1))\n","        e3 = self.enc3(self.pool(e2))\n","        e4 = self.enc4(self.pool(e3))\n","\n","        # デコーダパス：特徴統合とアップサンプリング（スキップ接続を使用）\n","        d3 = self.dec3(torch.cat([self.up(e4), e3], dim=1))\n","        d2 = self.dec2(torch.cat([self.up(d3), e2], dim=1))\n","        d1 = self.dec1(torch.cat([self.up(d2), e1], dim=1))\n","\n","        return self.final(d1)\n"]},{"cell_type":"markdown","metadata":{"id":"vzcggBG-8MnN"},"source":["# Train and Valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9YiGhXjB9IK"},"outputs":[],"source":["# config\n","@dataclass\n","class TrainingConfig:\n","    # データセットパス\n","    dataset_root: str = \"data\"\n","\n","    # データ関連\n","    batch_size: int = 32\n","    num_workers: int = 4\n","\n","    # モデル関連\n","    in_channels: int = 3\n","    num_classes: int = 13  # NYUv2データセットの場合\n","\n","    # 学習関連\n","    epochs: int = 100\n","    learning_rate: float = 0.001\n","    weight_decay: float = 1e-4\n","\n","    # データ分割関連\n","    train_val_split: float = 0.8  # 訓練データの割合\n","\n","    # デバイス設定\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    # チェックポイント関連\n","    checkpoint_dir: str = \"checkpoints\"\n","    save_interval: int = 5  # エポックごとのモデル保存間隔\n","\n","    # データ拡張・前処理関連\n","    image_size: tuple = (256, 256)\n","    normalize_mean: tuple = (0.485, 0.456, 0.406)  # ImageNetの標準化パラメータ\n","    normalize_std: tuple = (0.229, 0.224, 0.225)\n","\n","    def __post_init__(self):\n","        import os\n","        os.makedirs(self.checkpoint_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0OFbNO2DsCR"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"\n","    シードを固定する．\n","\n","    Parameters\n","    ----------\n","    seed : int\n","        乱数生成に用いるシード値．\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pF4K05BI7wDH"},"outputs":[],"source":["set_seed(42)\n","# 設定の初期化\n","config = TrainingConfig(\n","    dataset_root='/content/data',\n","    batch_size=16,\n","    num_workers=4,\n","    learning_rate=1e-4,\n","    epochs=100,\n","    image_size=(320, 240),\n","    in_channels=4  # RGB(3チャネル) + Depth(1チャネル)\n",")\n","\n","'''\n","データセットのディレクトリ構造：\n","    data/NYUv2/\n","    ├─train/\n","    │  ├─image/      # RGB画像（入力）\n","    │  │    000000.png\n","    │  │    ...\n","    |  ├─depth/      # 深度画像（入力）\n","    |  │    000000.png\n","    |  │    ...\n","    │  └─label/      # 13クラスセグメンテーション（教師ラベル）\n","    │       000000.png\n","    │       ...\n","    └─test/\n","       ├─image/      # RGB画像（入力）\n","       │    000000.png\n","       │    ...\n","       ├─depth/      # 深度画像（入力）\n","       │    000000.png\n","       │    ...\n","'''\n","\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","\n","# データ前処理の定義\n","# RGB画像のTransform：リサイズとテンソル変換\n","transform = Compose([\n","    Resize(config.image_size, interpolation=InterpolationMode.BILINEAR),\n","    ToTensor()\n","])\n","\n","# セグメンテーションラベルのTransform：リサイズとテンソル変換\n","target_transform = Compose([\n","    Resize(config.image_size, interpolation=InterpolationMode.NEAREST),\n","    Lambda(lambda lbl: torch.from_numpy(np.array(lbl)).long())\n","])\n","\n","# データセットの準備\n","# RGBデータセットとセグメンテーションラベルの読み込み\n","train_dataset = NYUv2(\n","    root=config.dataset_root,\n","    split='train',\n","    include_depth=True,\n","    transform=transform,\n","    target_transform=target_transform\n",")\n","\n","# テストデータセット\n","test_dataset = NYUv2(\n","    root=config.dataset_root,\n","    split='test',\n","    include_depth=True,\n","    transform=transform\n",")\n","\n","\n","'''\n","    train data:\n","        Type of batch: tuple\n","        Index 0 (入力データ):\n","            Type: torch.Tensor\n","            Shape: torch.Size([Batch, 3, N, M])\n","            Details: RGBテンソル\n","                    - チャネル0-2: RGB画像 (値域: 0-1)\n","        Index 1 (教師ラベル):\n","            Type: torch.Tensor\n","            Shape: torch.Size([Batch, N, M])\n","            Details: セグメンテーションマップ\n","                    - 値域: 0-12 (13クラス)\n","                    - 255: ignore index\n","\n","    test data:\n","        Type of batch: torch.Tensor\n","        Shape: torch.Size([Batch, 3, N, M])\n","        Details: RGB画像 (値域: 0-1)\n","'''\n","\n","# データローダーの作成\n","train_data = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n","test_data = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=config.num_workers)\n","\n","# モデルとトレーニングの設定\n","device = config.device\n","print(f\"Using device: {device}\")\n","\n","# ------------------\n","#    Model\n","# ------------------\n","model = UNet(in_channels=config.in_channels, num_classes=config.num_classes).to(device)\n","\n","# ------------------\n","#    optimizer\n","# ------------------\n","optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","criterion = nn.CrossEntropyLoss(ignore_index=255)\n","\n","# ------------------\n","#    Training\n","# ------------------\n","num_epochs = config.epochs\n","scaler = GradScaler()\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    print(f\"on epoch: {epoch+1}\")\n","    with tqdm(train_data) as pbar:\n","        for batch_idx, (image, depth, label) in enumerate(pbar):\n","            image, depth, label = image.to(device), depth.to(device), label.to(device)\n","            optimizer.zero_grad()\n","\n","            with autocast():\n","              x = torch.cat((image, depth), dim=1) # RGB + Depth\n","              pred = model(x)\n","              loss = criterion(pred, label)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            total_loss += loss.item()\n","            del image, depth, label, pred, loss\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n","\n","# モデルの保存\n","current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","model_path = f\"model_{current_time}.pt\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uMIiVEdEEtG"},"outputs":[],"source":["# ------------------\n","#    Evaluation\n","# ------------------\n","\n","model.load_state_dict(torch.load(model_path, map_location=device))\n","model.eval()\n","\n","# 予測結果の生成\n","predictions = []\n","\n","with torch.no_grad():\n","    print(\"Generating predictions...\")\n","    for image, depth in tqdm(test_data):\n","        image, depth = image.to(device), depth.to(device)\n","        x = torch.cat((image, depth), dim=1)\n","        output = model(x)            # [Batch, num_classes, H, W]\n","        pred = output.argmax(dim=1)  # [Batch, H, W]\n","        predictions.append(pred.cpu())\n","predictions = torch.cat(predictions, dim=0)\n","\n","predictions = predictions.cpu().numpy()\n","np.save('submission.npy', predictions)\n","print(\"Predictions saved to submission.npy\")"]},{"cell_type":"markdown","metadata":{"id":"Cdlb2A8I_pUD"},"source":["## 提出方法\n","\n","以下の3点をzip化し，Omnicampusの「最終課題 (セグメンテーション)」から提出してください．\n","\n","- `submission.npy`\n","- `model.pt`や`model_best.pt`など，テストに使用した重み（拡張子は`.pt`のみ）\n","- 本Colab Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0vBGHo__oYw"},"outputs":[],"source":["from zipfile import ZipFile, ZIP_DEFLATED\n","\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/DL_Basic_2025_Competition_NYUv2_baseline.ipynb\"\n","\n","with ZipFile(\"submission.zip\",\n","             mode=\"w\",\n","             compression=ZIP_DEFLATED,\n","             compresslevel=9) as zf:\n","    zf.write(\"submission.npy\")\n","    zf.write(model_path)\n","    zf.write(notebook_path,\n","             arcname=\"DL_Basic_2025_Competition_NYUv2_baseline.ipynb\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jqb5TnRq_ITD"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}