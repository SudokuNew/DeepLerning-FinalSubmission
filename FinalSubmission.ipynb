{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9228c3ec",
   "metadata": {},
   "source": [
    "# DeepLerning最終課題\n",
    "\n",
    "松尾研DeepLerning深層学習Springの最終課題のマスターコードである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5aba4",
   "metadata": {},
   "source": [
    "## 0.概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896c510",
   "metadata": {},
   "source": [
    "#### 概要\n",
    "被験者が画像を見ているときの脳波から，その画像がどのカテゴリに属するかを分類するタスク．\n",
    "- サンプル数: 訓練 118,800 サンプル，検証 59,400 サンプル，テスト 59,400 サンプル\n",
    "- クラス数: 5\n",
    "- 入力: 脳波データ（チャンネル数 x 系列長）\n",
    "- 出力: 対応する画像のクラス\n",
    "- 評価指標: Top-1 accuracy\n",
    "\n",
    "#### 修了要件\n",
    "- ベースラインモデルのbest test accuracyは38.7%となります．**これを超えた提出のみ，修了要件として認めます**．\n",
    "- ベースラインから改善を加えることで，55%までは性能向上することを運営で確認しています．こちらを 1 つの指標として取り組んでみてください．\n",
    "\n",
    "#### 注意点\n",
    "- 学習するモデルについて制限はありませんが，必ず訓練データで学習したモデルで予測してください．\n",
    "    - 事前学習済みモデルを利用して，訓練データを fine-tuning しても構いません．\n",
    "    - 埋め込み抽出モデルなど，モデルの一部を訓練しないケースは構いません．\n",
    "    - 学習を一切せずに，ChatGPT などの基盤モデルを利用することは禁止とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609f9855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2171e431eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#インポート\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from glob import glob\n",
    "from termcolor import cprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6b934",
   "metadata": {},
   "source": [
    "# BaseCord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe8d83",
   "metadata": {},
   "source": [
    "## 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f7b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\最終課題\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sudok\\AppData\\Local\\Programs\\Python\\nova\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "WORK_DIR = r\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\最終課題\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "%cd {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7513396",
   "metadata": {},
   "source": [
    "## 2.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc210da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThingsEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 5\n",
    "        self.num_subjects = 10\n",
    "\n",
    "        self.X = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\eeg.npy\")\n",
    "        self.X = torch.from_numpy(self.X).to(torch.float32)\n",
    "        self.subject_idxs = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\subject_idxs.npy\")\n",
    "        self.subject_idxs = torch.from_numpy(self.subject_idxs)\n",
    "\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\labels.npy\")\n",
    "            self.y = torch.from_numpy(self.y)\n",
    "\n",
    "        print(f\"EEG: {self.X.shape}, labels: {self.y.shape if hasattr(self, 'y') else None}, subject indices: {self.subject_idxs.shape}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if hasattr(self, \"y\"):\n",
    "            return self.X[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4479c2",
   "metadata": {},
   "source": [
    "## 3.Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3a5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        kernel_size: int = 3,\n",
    "        p_drop: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.conv1 = nn.Conv1d(out_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        # self.conv2 = nn.Conv1d(out_dim, out_dim, kernel_size) # , padding=\"same\")\n",
    "\n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.out_dim:\n",
    "            X = self.conv0(X) + X  # skip connection\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "\n",
    "        X = self.conv1(X) + X  # skip connection\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "\n",
    "        # X = self.conv2(X)\n",
    "        # X = F.glu(X, dim=-2)\n",
    "\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "class BasicConvClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X)\n",
    "\n",
    "        return self.head(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c5c40",
   "metadata": {},
   "source": [
    "## 4.Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65634601",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m is_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#    Dataloader\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mThingsEEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ThingsMEGDataset(\"train\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     12\u001b[0m     train_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m val_set \u001b[38;5;241m=\u001b[39m ThingsEEGDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# ThingsMEGDataset(\"val\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mThingsEEGDataset.__init__\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_subjects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msudok\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mドキュメント\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m0B-DeepLerning\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDeepLerning-FinalSubmission\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFinalSubmission\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43meeg.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msudok\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mドキュメント\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m0B-DeepLerning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeepLerning-FinalSubmission\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFinalSubmission\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msubject_idxs.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sudok\\AppData\\Local\\Programs\\Python\\nova\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:460\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    458\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[0;32m    463\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "# ハイパラ\n",
    "lr = 0.001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "is_cuda = False\n",
    "\n",
    "# ------------------\n",
    "#    Dataloader\n",
    "# ------------------\n",
    "train_set = ThingsEEGDataset(\"train\") # ThingsMEGDataset(\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_set = ThingsEEGDataset(\"val\") # ThingsMEGDataset(\"val\")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "if is_cuda:\n",
    "    model = BasicConvClassifier(\n",
    "        train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    "    ).to(\"cuda\")\n",
    "else:\n",
    "    model = BasicConvClassifier(\n",
    "        train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    "    )\n",
    "\n",
    "# ------------------\n",
    "#     Optimizer\n",
    "# ------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ------------------\n",
    "#   Start training\n",
    "# ------------------\n",
    "max_val_acc = 0\n",
    "def accuracy(y_pred, y):\n",
    "    return (y_pred.argmax(dim=-1) == y).float().mean()\n",
    "\n",
    "writer = SummaryWriter(\"tensorboard\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "    model.train()\n",
    "    for X, y, subject_idxs in tqdm(train_loader, desc=\"Train\"):\n",
    "        if is_cuda:\n",
    "            X, y = X.to(\"cuda\"), y.to(\"cuda\")\n",
    "        else:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc.append(acc.item())\n",
    "\n",
    "    model.eval()\n",
    "    for X, y, subject_idxs in tqdm(val_loader, desc=\"Validation\"):\n",
    "        if is_cuda:\n",
    "            X, y = X.to(\"cuda\"), y.to(\"cuda\")\n",
    "        else:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "\n",
    "        val_loss.append(F.cross_entropy(y_pred, y).item())\n",
    "        val_acc.append(accuracy(y_pred, y).item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \\\n",
    "        train loss: {np.mean(train_loss):.3f} | \\\n",
    "        train acc: {np.mean(train_acc):.3f} | \\\n",
    "        val loss: {np.mean(val_loss):.3f} | \\\n",
    "        val acc: {np.mean(val_acc):.3f}\")\n",
    "\n",
    "    writer.add_scalar(\"train_loss\", np.mean(train_loss), epoch)\n",
    "    writer.add_scalar(\"train_acc\", np.mean(train_acc), epoch)\n",
    "    writer.add_scalar(\"val_loss\", np.mean(val_loss), epoch)\n",
    "    writer.add_scalar(\"val_acc\", np.mean(val_acc), epoch)\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_last.pt\")\n",
    "\n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        cprint(\"New best. Saving the model.\", \"cyan\")\n",
    "        torch.save(model.state_dict(), \"model_best.pt\")\n",
    "        max_val_acc = np.mean(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d14177",
   "metadata": {},
   "source": [
    "## 5.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "#    Dataloader\n",
    "# ------------------\n",
    "test_set = ThingsEEGDataset(\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "model = BasicConvClassifier(\n",
    "    test_set.num_classes, test_set.seq_len, test_set.num_channels\n",
    ").to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"model_best.pt\", map_location=\"cuda\"))\n",
    "\n",
    "# ------------------\n",
    "#  Start evaluation\n",
    "# ------------------\n",
    "preds = []\n",
    "model.eval()\n",
    "for X, subject_idxs in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "    preds.append(model(X.to(\"cuda\")).detach().cpu())\n",
    "\n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "np.save(\"submission\", preds)\n",
    "print(f\"Submission {preds.shape} saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe251000",
   "metadata": {},
   "source": [
    "# OriginalCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0805f24",
   "metadata": {},
   "source": [
    "## 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc2c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThingsEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 5\n",
    "        self.num_subjects = 10\n",
    "\n",
    "        self.X = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\eeg.npy\")\n",
    "        self.X = torch.from_numpy(self.X).to(torch.float32)\n",
    "        self.subject_idxs = np.load(fr\"CC:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\subject_idxs.npy\")\n",
    "        self.subject_idxs = torch.from_numpy(self.subject_idxs)\n",
    "\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\labels.npy\")\n",
    "            self.y = torch.from_numpy(self.y)\n",
    "\n",
    "        print(f\"EEG: {self.X.shape}, labels: {self.y.shape if hasattr(self, 'y') else None}, subject indices: {self.subject_idxs.shape}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if hasattr(self, \"y\"):\n",
    "            return self.X[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ab75ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mThingsEEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ThingsMEGDataset(\"train\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m val_set \u001b[38;5;241m=\u001b[39m ThingsEEGDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# ThingsMEGDataset(\"val\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mThingsEEGDataset.__init__\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_subjects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msudok\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mドキュメント\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m0B-DeepLerning\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDeepLerning-FinalSubmission\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFinalSubmission\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43meeg.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msudok\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mドキュメント\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m0B-DeepLerning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeepLerning-FinalSubmission\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFinalSubmission\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msubject_idxs.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sudok\\AppData\\Local\\Programs\\Python\\nova\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:460\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    458\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[0;32m    463\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "train_set = ThingsEEGDataset(\"train\") # ThingsMEGDataset(\"train\")\n",
    "val_set = ThingsEEGDataset(\"val\") # ThingsMEGDataset(\"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
