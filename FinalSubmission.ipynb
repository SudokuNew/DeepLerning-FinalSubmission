{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9228c3ec",
   "metadata": {},
   "source": [
    "# DeepLerning最終課題\n",
    "\n",
    "松尾研DeepLerning深層学習Springの最終課題のマスターコードである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5aba4",
   "metadata": {},
   "source": [
    "## 0.概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896c510",
   "metadata": {},
   "source": [
    "#### 概要\n",
    "被験者が画像を見ているときの脳波から，その画像がどのカテゴリに属するかを分類するタスク．\n",
    "- サンプル数: 訓練 118,800 サンプル，検証 59,400 サンプル，テスト 59,400 サンプル\n",
    "- クラス数: 5\n",
    "- 入力: 脳波データ（チャンネル数 x 系列長）\n",
    "- 出力: 対応する画像のクラス\n",
    "- 評価指標: Top-1 accuracy\n",
    "\n",
    "#### 修了要件\n",
    "- ベースラインモデルのbest test accuracyは38.7%となります．**これを超えた提出のみ，修了要件として認めます**．\n",
    "- ベースラインから改善を加えることで，55%までは性能向上することを運営で確認しています．こちらを 1 つの指標として取り組んでみてください．\n",
    "\n",
    "#### 注意点\n",
    "- 学習するモデルについて制限はありませんが，必ず訓練データで学習したモデルで予測してください．\n",
    "    - 事前学習済みモデルを利用して，訓練データを fine-tuning しても構いません．\n",
    "    - 埋め込み抽出モデルなど，モデルの一部を訓練しないケースは構いません．\n",
    "    - 学習を一切せずに，ChatGPT などの基盤モデルを利用することは禁止とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609f9855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a0ff7b20f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#インポート\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from glob import glob\n",
    "from termcolor import cprint\n",
    "from tqdm.notebook import tqdm\n",
    "from torchaudio.models import Conformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "\n",
    "is_jupyter = False\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install gdown if not already installed\n",
    "if is_jupyter:\n",
    "    !pip install gdown\n",
    "    !pip install --upgrade gdown\n",
    "    folder_id = '1qkxOgiD1Z0U5DnaYhs3RSfWdQHvzE4HX'\n",
    "    !gdown --folder {folder_id}\n",
    "\n",
    "    file_id = '1QNZv_4s89Lq8bwc6XNsqKKpxYZlLPWHT'\n",
    "    !gdown https://drive.google.com/uc?id={file_id} -O downloaded.tar.gz\n",
    "    !tar -xzvf downloaded.tar.gz -C data/\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6b934",
   "metadata": {},
   "source": [
    "# BaseCord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe8d83",
   "metadata": {},
   "source": [
    "## 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f7b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\最終課題\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sudok\\AppData\\Local\\Programs\\Python\\nova\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "WORK_DIR = r\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\最終課題\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "%cd {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7513396",
   "metadata": {},
   "source": [
    "## 2.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc210da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThingsEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 5\n",
    "        self.num_subjects = 10\n",
    "\n",
    "        self.X = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\eeg.npy\")\n",
    "        self.X = torch.from_numpy(self.X).to(torch.float32)\n",
    "        self.subject_idxs = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\subject_idxs.npy\")\n",
    "        self.subject_idxs = torch.from_numpy(self.subject_idxs)\n",
    "\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\labels.npy\")\n",
    "            self.y = torch.from_numpy(self.y)\n",
    "\n",
    "        print(f\"EEG: {self.X.shape}, labels: {self.y.shape if hasattr(self, 'y') else None}, subject indices: {self.subject_idxs.shape}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if hasattr(self, \"y\"):\n",
    "            return self.X[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4479c2",
   "metadata": {},
   "source": [
    "## 3.Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3a5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        kernel_size: int = 3,\n",
    "        p_drop: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.conv1 = nn.Conv1d(out_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        # self.conv2 = nn.Conv1d(out_dim, out_dim, kernel_size) # , padding=\"same\")\n",
    "\n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.out_dim:\n",
    "            X = self.conv0(X) + X  # skip connection\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "\n",
    "        X = self.conv1(X) + X  # skip connection\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "\n",
    "        # X = self.conv2(X)\n",
    "        # X = F.glu(X, dim=-2)\n",
    "\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "class BasicConvClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X)\n",
    "\n",
    "        return self.head(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c5c40",
   "metadata": {},
   "source": [
    "## 4.Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65634601",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m is_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#    Dataloader\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mThingsEEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ThingsMEGDataset(\"train\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     12\u001b[0m     train_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m val_set \u001b[38;5;241m=\u001b[39m ThingsEEGDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# ThingsMEGDataset(\"val\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mThingsEEGDataset.__init__\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_subjects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msudok\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mドキュメント\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m0B-DeepLerning\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDeepLerning-FinalSubmission\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFinalSubmission\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43meeg.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msudok\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mドキュメント\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m0B-DeepLerning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeepLerning-FinalSubmission\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFinalSubmission\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msubject_idxs.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sudok\\AppData\\Local\\Programs\\Python\\nova\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:460\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    458\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[0;32m    463\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "# ハイパラ\n",
    "lr = 0.001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "is_cuda = False\n",
    "\n",
    "# ------------------\n",
    "#    Dataloader\n",
    "# ------------------\n",
    "train_set = ThingsEEGDataset(\"train\") # ThingsMEGDataset(\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_set = ThingsEEGDataset(\"val\") # ThingsMEGDataset(\"val\")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "if is_cuda:\n",
    "    model = BasicConvClassifier(\n",
    "        train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    "    ).to(\"cuda\")\n",
    "else:\n",
    "    model = BasicConvClassifier(\n",
    "        train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    "    )\n",
    "\n",
    "# ------------------\n",
    "#     Optimizer\n",
    "# ------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ------------------\n",
    "#   Start training\n",
    "# ------------------\n",
    "max_val_acc = 0\n",
    "def accuracy(y_pred, y):\n",
    "    return (y_pred.argmax(dim=-1) == y).float().mean()\n",
    "\n",
    "writer = SummaryWriter(\"tensorboard\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "    model.train()\n",
    "    for X, y, subject_idxs in tqdm(train_loader, desc=\"Train\"):\n",
    "        if is_cuda:\n",
    "            X, y = X.to(\"cuda\"), y.to(\"cuda\")\n",
    "        else:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc.append(acc.item())\n",
    "\n",
    "    model.eval()\n",
    "    for X, y, subject_idxs in tqdm(val_loader, desc=\"Validation\"):\n",
    "        if is_cuda:\n",
    "            X, y = X.to(\"cuda\"), y.to(\"cuda\")\n",
    "        else:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "\n",
    "        val_loss.append(F.cross_entropy(y_pred, y).item())\n",
    "        val_acc.append(accuracy(y_pred, y).item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \\\n",
    "        train loss: {np.mean(train_loss):.3f} | \\\n",
    "        train acc: {np.mean(train_acc):.3f} | \\\n",
    "        val loss: {np.mean(val_loss):.3f} | \\\n",
    "        val acc: {np.mean(val_acc):.3f}\")\n",
    "\n",
    "    writer.add_scalar(\"train_loss\", np.mean(train_loss), epoch)\n",
    "    writer.add_scalar(\"train_acc\", np.mean(train_acc), epoch)\n",
    "    writer.add_scalar(\"val_loss\", np.mean(val_loss), epoch)\n",
    "    writer.add_scalar(\"val_acc\", np.mean(val_acc), epoch)\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_last.pt\")\n",
    "\n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        cprint(\"New best. Saving the model.\", \"cyan\")\n",
    "        torch.save(model.state_dict(), \"model_best.pt\")\n",
    "        max_val_acc = np.mean(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d14177",
   "metadata": {},
   "source": [
    "## 5.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "#    Dataloader\n",
    "# ------------------\n",
    "test_set = ThingsEEGDataset(\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "model = BasicConvClassifier(\n",
    "    test_set.num_classes, test_set.seq_len, test_set.num_channels\n",
    ").to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"model_best.pt\", map_location=\"cuda\"))\n",
    "\n",
    "# ------------------\n",
    "#  Start evaluation\n",
    "# ------------------\n",
    "preds = []\n",
    "model.eval()\n",
    "for X, subject_idxs in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "    preds.append(model(X.to(\"cuda\")).detach().cpu())\n",
    "\n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "np.save(\"submission\", preds)\n",
    "print(f\"Submission {preds.shape} saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe251000",
   "metadata": {},
   "source": [
    "# OriginalCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0805f24",
   "metadata": {},
   "source": [
    "## 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc2c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThingsEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 5\n",
    "        self.num_subjects = 10\n",
    "\n",
    "        self.X = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\eeg.npy\")\n",
    "        self.X = torch.from_numpy(self.X).to(torch.float32)\n",
    "        self.subject_idxs = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\subject_idxs.npy\")\n",
    "        self.subject_idxs = torch.from_numpy(self.subject_idxs)\n",
    "\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = np.load(fr\"C:\\Users\\sudok\\OneDrive\\ドキュメント\\0B-DeepLerning\\DeepLerning-FinalSubmission\\FinalSubmission\\data\\{split}\\labels.npy\")\n",
    "            self.y = torch.from_numpy(self.y)\n",
    "\n",
    "        print(f\"EEG: {self.X.shape}, labels: {self.y.shape if hasattr(self, 'y') else None}, subject indices: {self.subject_idxs.shape}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if hasattr(self, \"y\"):\n",
    "            return self.X[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab75ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG: torch.Size([118800, 17, 100]), labels: torch.Size([118800]), subject indices: torch.Size([118800])\n",
      "EEG: torch.Size([59400, 17, 100]), labels: torch.Size([59400]), subject indices: torch.Size([59400])\n"
     ]
    }
   ],
   "source": [
    "train_set = ThingsEEGDataset(\"train\")\n",
    "val_set = ThingsEEGDataset(\"val\")\n",
    "test_set = ThingsEEGDataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c9690",
   "metadata": {},
   "source": [
    "## 2.精度向上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcffe0c",
   "metadata": {},
   "source": [
    "### **精度向上への施策**\n",
    "\n",
    "**・Conformerによる音声認識**\n",
    ">音声認識のConformerを脳波予測に応用し、精度向上を図る"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783293eb",
   "metadata": {},
   "source": [
    "## 3.Conformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb5737",
   "metadata": {},
   "source": [
    "**Conformerによる音声認識のTransfromerの予測**\n",
    "\n",
    "\n",
    ">end-to-endの音声認識システムは、RNNをベースとしたモデルによって発展してきた。また、最近では、self-attentionによるTransformerベースのモデルが、様々な\n",
    ">領域へと展開されてきている。一方で、音声認識においてはlocalのcontextを捉えることができるCNNが有用であり、いまだによく使われている。\n",
    ">Transformerは、globalの相互作用を捉えやすいが、localの特徴を抽出しにくい。また、CNNはkernelの位置と移動距離によって局所的な特徴量を抽出していくため、\n",
    ">globalの相互作用を捉えるのは難しい。ContextNetは、CNNベースでありながら、残差blockを活用することでglobalの特徴を抽出していたが十分とは言えない。\n",
    ">ただ最近では、CNNとself-attentionを組み合わせることによって、それぞれを個別に扱うよりも性能が向上することが様々な研究によってわかってきている。\n",
    ">本論では、音声認識モデルにおいて、CNNにself-attentionを結合した独自の方法を紹介する。また、globalとlocalの相互作用を扱うことで、パラメータの効率化も\n",
    ">可能であると想定している。\n",
    "\n",
    "\n",
    "![Transformer structure diagram](https://storage.googleapis.com/zenn-user-upload/1fc351a21052-20230206.png \"Transformer\")\n",
    "\n",
    "[【論文紹介】Conformer](https://zenn.dev/nudibranch/articles/577a0bb1fab32a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af05dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConformer(nn.Module):\n",
    "    def __init__(self, num_classes, input_dim, num_layers=4, hidden_dim=144, num_heads=4, ffmult=4, dropout=0.1):\n",
    "        super(EEGConformer, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        EEG向けConformerモデル\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): 分類クラス数\n",
    "            input_dim (int): 各時刻の特徴量数（EEGならチャンネル数）\n",
    "            num_layers (int): Conformer層の数\n",
    "            hidden_dim (int): 内部の隠れ層サイズ\n",
    "            num_heads (int): Self-Attentionのヘッド数\n",
    "            ff_mult (int): Feed Forwardの拡張倍率\n",
    "            dropout (float): ドロップアウト率\n",
    "        \"\"\"\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.conformer = Conformer(\n",
    "            input_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            ffn_dim=hidden_dim * ffmult,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            depthwise_conv_kernel_size=31  # 例として31を指定\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = [batch_size, seq_len, input_dim]\n",
    "        \"\"\"\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        lengths = torch.full((batch_size,), seq_len, dtype=torch.int64, device=x.device)\n",
    "        \n",
    "        x, _ = self.conformer(x, lengths)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebaf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "barch_size = 32\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=barch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=barch_size, shuffle=False)\n",
    "\n",
    "model_conformer = EEGConformer(input_dim=train_set.num_channels,\n",
    "                               num_classes=train_set.num_classes, \n",
    "                               num_layers=4,\n",
    "                               hidden_dim=144,\n",
    "                               num_heads=4,\n",
    "                               ffmult=4,\n",
    "                               dropout=0.1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_conformer.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_conformer.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X, y, subject_idxs in tqdm(train_loader, f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        X = X.permute(0, 2, 1) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_conformer(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    model_conformer.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y, subject_idxs in tqdm(val_loader, f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            X = X.permute(0, 2, 1) \n",
    "\n",
    "            outputs = model_conformer(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == y).sum().item()\n",
    "            val_total += y.size(0)\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f},\"\n",
    "          f\"LR: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#推論\n",
    "test_set = ThingsEEGDataset(\"test\")\n",
    "test_loader = DataLoader(test_set, batch_size=barch_size, shuffle=False)\n",
    "\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_conformer = EEGConformer(input_dim=train_set.num_channels,\n",
    "                               num_classes=train_set.num_classes, \n",
    "                               num_layers=4,\n",
    "                               hidden_dim=144,\n",
    "                               num_heads=4,\n",
    "                               ffmult=4,\n",
    "                               dropout=0.1).to(device)\n",
    "model_conformer.load_state_dict(torch.load(\"conformer_weights.pth\"))\n",
    "model_conformer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X = X.permute(0, 2, 1)\n",
    "\n",
    "        outputs = model_conformer(X)\n",
    "        all_outputs.append(outputs.cpu())\n",
    "        all_labels.append(y.cpu())\n",
    "\n",
    "# 結合\n",
    "all_outputs = torch.cat(all_outputs, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "np.save(\"submission.npy\", all_outputs)\n",
    "print(f\"Submission {all_outputs.shape} saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94af361",
   "metadata": {},
   "source": [
    "行った修正：\n",
    "\n",
    "・StepLRやReduceLROnPlateauなどのスケジューラーの導入\n",
    ">\n",
    ">``StepLR``：Epochsごとに学習率を変動させる\n",
    ">```python\n",
    ">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_epochs, gamma=lrning_late)\n",
    ">```\n",
    "\n",
    ">``ReduceLROPlateau``：性能の停滞時、学習率を変動させる\n",
    ">```python\n",
    ">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=decrease_late, patience=step_epochs, verbose=True)\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a25c84",
   "metadata": {},
   "source": [
    "## 4.CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557ecc1",
   "metadata": {},
   "source": [
    "**CLIPによる応用的な脳波画像分類**\n",
    "\n",
    "\n",
    ">CLIPの基本的なアイデアは，言語情報（テキスト）とペアになっている画像の対照学習によって，よい言語と画像の表現（embedding）を学習する点にあります\n",
    ">上記のように事前学習されたCLIPは，画像とテキストが対応するような表現（共有表現）が学習されていると考えられます． このモデルを画像分類に活用する場合\n",
    ">を考えてみましょう（図2）．\n",
    ">飛行機（plane），自動車（car），…の分類器を作る場合は，まずそれぞれの画像を説明するテキスト（a photo of plane, a photo of car, …）のembeddingを作成\n",
    ">します（図2上部）．\n",
    ">そして，分類したい画像を画像のエンコーダに入力した際のembeddingと，それぞれのテキストとのコサイン類似度を計算し，最大になるクラスを選択することで，\n",
    ">分類ができます（図2下部）．\n",
    ">このようにして，対象となるdownstreamの分類タスクに関するデータセットを集めてfine-tuningすることなしに，zero-shotで目的のデータセットに関する分類器を\n",
    ">（理想的には）構成できます。\n",
    "\n",
    "![CLIP's Pre-study mechanism](https://trail.t.u-tokyo.ac.jp/ja/blog/22-12-02-clip/overview-a.svg \"CLIP's prestudy\")\n",
    "\n",
    "[CLIP：言語と画像のマルチモーダル基盤モデル](https://trail.t.u-tokyo.ac.jp/ja/blog/22-12-02-clip/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50eac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGWithImageDataset(Dataset):\n",
    "    def __init__(self, eeg_dataset, image_dir, transform=None):\n",
    "        self.eeg_dataset = eeg_dataset\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_folders = sorted(os.listdir(self.image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eeg_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg, label, subj = self.eeg_dataset[idx]\n",
    "\n",
    "        folder_name = self.image_folders[label.item()]\n",
    "        folder_path = os.path.join(self.image_dir, folder_name)\n",
    "\n",
    "        image_files = sorted(os.listdir(folder_path))\n",
    "        image_name = image_files[0]\n",
    "\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return eeg, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG: torch.Size([118800, 17, 100]), labels: torch.Size([118800]), subject indices: torch.Size([118800])\n",
      "EEG: torch.Size([59400, 17, 100]), labels: torch.Size([59400]), subject indices: torch.Size([59400])\n",
      "EEG: torch.Size([118800, 17, 100]), labels: None, subject indices: torch.Size([59400])\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),              # 画像サイズを224x224にリサイズ\n",
    "    transforms.CenterCrop(224),          # 中心部分をクロップ\n",
    "    transforms.ToTensor(),               # Tensor型に変換\n",
    "    transforms.Normalize(                # 標準化 (CLIPと同じ値を使う場合はCLIPの値を使う)\n",
    "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "        std=[0.26862954, 0.26130258, 0.27577711]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = EEGWithImageDataset(train_set, image_dir=\"data/images\", transform=preprocess)\n",
    "val_dataset = EEGWithImageDataset(val_set, image_dir=\"data/images\", transform=preprocess)\n",
    "test_dataset = EEGWithImageDataset(test_set, image_dir=\"data/images\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52100af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, eeg_model):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            *eeg_model.conformer_layers[:-1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        for module in self.features:\n",
    "            x = module(x, key_padding_mask=None)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d98bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model_clip, _, _ = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"openai\", force_quick_gelu=True\n",
    ")\n",
    "model_clip.eval()\n",
    "model_clip = model_clip.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc46d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#全結合\n",
    "class EEG_CLIP_ConcatModel(nn.Module):\n",
    "    def __init__(self, eeg_feature_dim, clip_feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.eeg_feature_dim = eeg_feature_dim\n",
    "        self.clip_feature_dim = clip_feature_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(eeg_feature_dim + clip_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, eeg_feat, clip_feat):\n",
    "        combined = torch.cat([eeg_feat, clip_feat], dim=1)\n",
    "        out = self.classifier(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ecf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "concat_model = EEG_CLIP_ConcatModel(\n",
    "    eeg_feature_dim=17,\n",
    "    clip_feature_dim=512,\n",
    "    num_classes=train_dataset.eeg_dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "eeg_model = Conformer(\n",
    "    input_dim=train_set.num_channels, # num_channelsプロパティを使用\n",
    "    num_heads=4,\n",
    "    ffn_dim=128,\n",
    "    num_layers=4,\n",
    "    depthwise_conv_kernel_size=31,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "eeg_feature_extractor = EEGFeatureExtractor(eeg_model).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(eeg_feature_extractor.parameters()) + list(concat_model.parameters()), \n",
    "    lr=1e-4\n",
    ").to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    concat_model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for eeg, image, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        eeg = eeg.to(device)\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        eeg_feat = eeg_feature_extractor.to(device)\n",
    "        clip_feat = model_clip.encode_image(image)\n",
    "\n",
    "        output = concat_model(eeg_feat, clip_feat)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = output.argmax(dim=1)  # 予測ラベル\n",
    "        correct += (preds == label).sum().item()  # 正解数を加算\n",
    "        total += label.size(0)  # バッチサイズを加算\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f} Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
